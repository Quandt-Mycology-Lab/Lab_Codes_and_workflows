Your node, "bnode0510", is ready! Details:
 
1. You and your group members now have non-preemptable access to your node, which means you can run jobs of up to 7 days in length without interruption. Your node has 56 cores and 500 G of RAM. Note that presently the node has 2x hyperthreading enabled, which allows up to 2 tasks to share a single core. What this means fis that Slurm "sees" 56*2=112 cores on the node. Hyperthreading typically leads to more efficient jobs because it takes advantage of cores that aren't being 100% used by allowing jobs to share them. If desired, we can disable this feature (which would be equivalent to the 1x hyperthreading on Summit and Alpine); just let us know your preference.
 
Your node can be accessed by adding the following flags to your job scripts, respectively:
 
#SBATCH --partition=blanca-qsmicrobes
#SBATCH --account=blanca-qsmicrobes
#SBATCH --qos=blanca-qsmicrobes
 
2. In addition to having priority access to your node, you also have preemptable access to all ~200 Blanca nodes. This means that you can run jobs of up to 24 hours on any nodes on Blanca for which you don't have priority access (i.e., other groups' nodes). The jobs will be preempted if those groups' members need overlapping resources when your job is running, however your preempted job will be requeued and will eventually run. This is a fantastic way to leverage resources beyond nodes you have priority access to. To run a preemptable job, specify this in your job script:
 
#SBATCH --partition=<name-of-partition-you-want-to-run-on>
#SBATCH --account=blanca-curc
#SBATCH --qos=preemptable
 
...where <name-of-partition-you-want-to-run-on> can be obtained by browsing the available partitions (this can be done by typing "sinfo -a" on a login node after you've loaded the "slurm/blanca" module)). You can also see the characteristics of specific nodes (including the partition they are in), by typing "scontrol show node" on a login node after you've loaded the "slurm/blanca" module)
 
3. A general note on job scheduling for Blanca: All blanca jobs are scheduled from the same login nodes that you use to schedule Alpine jobs. The difference is that before using "sbatch" or "sinteractive" to start a job, you'll first need to "module load slurm/blanca" on the login node, in order to activate the Blanca Slurm environment.
 
4. The Blanca nodes differ from Alpine in that they presently have the RedHat7 operating system, whereas Alpine has Redhat8. What this means is that Blanca uses the older software modules that were present on Summit (which was also a RedHat7 system). This year we are planning to update the Blanca nodes to Redhat8, after which they will share the same modules with Alpine. In the meantime, workflows that were previously used on Summit should be valid on Blanca. I can assist where needed with transitioning workflows; otherwise, simply updating Summit jobs scripts with the partition/account/qos noted above is probably the most straightforward way to get started on Blanca.

5. You cannot use /scratch/ . You must use /rc_scratch/identikey/
